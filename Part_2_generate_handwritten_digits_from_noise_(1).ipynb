{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part_2_generate_handwritten_digits_from_noise (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhiJ2706/generate-images-AI/blob/master/Part_2_generate_handwritten_digits_from_noise_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIQsaAvd-e8u"
      },
      "source": [
        "**Generative Networks (part 2)- generating a picture of a digit from noise**\n",
        "\n",
        "This neural network can generate pictures of the number 8, given scranbled nonsense (noise) as input. The input would look like nothing in particular to a human, but the output is a picture of the number 8.\n",
        "\n",
        "*How it works*\n",
        "\n",
        "This form of neural network is called a GAN-  a Gain-Adversial Network.\n",
        "\n",
        "The neural network consists of 2 (sub)networks- the generator and the discriminator. A given loop through the network consists of this: \n",
        "- The generator (untrained) tries to create pictures of the number 8 (it creates noise instead)\n",
        "- The discriminator is trained to recognize the number 8 on a very small batch of images (128 images of the number 8 and 128 pictures of noise created by the untrained generator). \n",
        "- The discriminator then becomes untrainable\n",
        "- The generator is then trained to create pictures of the number 8 given noise. The discriminator examines each output the generator creates to check if it indeed looks like the number 8 or not. \n",
        "\n",
        "This process is repeated 1050 times to generate the output. \n",
        "\n",
        "*Data formatting*\n",
        "\n",
        "At the beginning of this project I was training the discriminator once on 16000 data points, and then the generator once on 10000 data points. This did no return good results\n",
        "\n",
        "This barely gets the shape of an 8, but most of it is really just noise. Formatting the data to just use 128 images of 8 and 128 noise and then training it many more times gave better results.\n",
        "\n",
        "*Epoch Number, Dropout and Learning Rate*\n",
        "\n",
        "The number of Epochs is the amount of times you repeat the training process. In the original network (before the data was formatted), the discriminator ran 10 epochs once and then the generator ran 75 epochs, once. Now, both run 1050 in in sequence (so the discriminator runs 1 epoch then the generator, and this is repeated 1050 times). The number 1050 was chosen by testing various numbers of epochs on the data and seeing how close the result was.\n",
        "\n",
        "Dropout is a neural network layer which sets a random fraction of the inputs to the next layer to 0. This is important because it greatly increases the ability for the network to learn seeing as it has less values to concern itself with. Adding a few dropouts to the generator and discriminator greatly increased their ability to recognize shapes. \n",
        "\n",
        "The learning rate is part of a neural network optimizer, an algorithm that makes learning easier on the network. One of its changeable parameters is called the learning rate. Changing this changes the amount by which the weights of the network are altered. Having a learning rate that's too high or low can result in unusable output.\n",
        "\n",
        "Configuring these parameters correctly can result in good output.\n",
        "\n",
        "You can run the code below to see this in action!\n",
        "\n",
        "Abhi\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXAcvZY6LWKa"
      },
      "source": [
        "#CHANGE THIS TO CHANGE THE DIGIT BEING DRAWN\n",
        "n = 8"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pKJpjF1xsXM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "4f6ca8c9-d871-426c-bf10-36203104b38b"
      },
      "source": [
        "#import libraries\n",
        "#machine learning libraries\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, LeakyReLU, Dropout\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "#other libraries\n",
        "import numpy as np\n",
        "import random\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [10, 8] #configuring graphs \n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data() #getting train data\n",
        "\n",
        "#making train data and test data neural network friendly (converts images from 28x28 to 1x784)\n",
        "train_images = train_images.reshape((train_images.shape[0], 784)).astype('float32')\n",
        "train_images = train_images / 255\n",
        "\n",
        "test_images = test_images.reshape((test_images.shape[0], 784)).astype('float32')\n",
        "test_images = test_images / 255\n",
        "\n",
        "#Getting pictures of the number 8\n",
        "realData = []\n",
        "\n",
        "for i in range(len(train_images)):\n",
        "  if train_labels[i] == n and len(realData) < 128:\n",
        "    realData.append(train_images[i])\n",
        "\n",
        "realAnswers = [n for i in range(len(realData))]\n",
        "realData = np.array(realData, dtype=\"float\")\n",
        "realAnswers = np.array(realAnswers, dtype=\"float\")\n",
        "\n",
        "print(realData.shape, realAnswers.shape)\n",
        "\n",
        "print(train_images[61].shape)\n",
        "\n",
        "#Generator network\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(256,input_dim=784, activation=tf.nn.relu), #input\n",
        "    #hidden layers\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(432),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.LeakyReLU(0.2),\n",
        "    keras.layers.Dense(1024, activation=tf.nn.leaky_relu),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.LeakyReLU(0.2),\n",
        "    keras.layers.Dense(432),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.LeakyReLU(0.2),\n",
        "    keras.layers.Dense(784, activation=tf.nn.leaky_relu) #output layer\n",
        "])\n",
        "\n",
        "#discriminator network\n",
        "model2 = keras.Sequential([\n",
        "    keras.layers.Dense(784, input_dim=784, activation = tf.nn.tanh), #input\n",
        "    #hidden layers\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(256, activation=tf.nn.tanh),\n",
        "    keras.layers.Dense(60, activation=tf.nn.tanh),\n",
        "    keras.layers.Dense(30, activation=tf.nn.tanh),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(10, activation=tf.nn.softmax) #output\n",
        "])\n",
        "\n",
        "#compiling models with regular optimizers\n",
        "model.compile(optimizer='adam',\n",
        "              loss='mean_squared_error',\n",
        "              metrics=[\"mean_squared_error\", \"accuracy\"])\n",
        "\n",
        "model2.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "adam = Adam(lr=0.000327, beta_1=0.5) #custom optimizer\n",
        "\n",
        "#Creating the GAN by joing the generator and discriminator\n",
        "model2.trainable = False\n",
        "ganInput = Input(shape=(784,)) #creating input layer for GAN\n",
        "x = model(ganInput) #giving this input layer to generator\n",
        "ganOutput = model2(x) #giving the generator with the new input layer to the discriminator\n",
        "gan = Model(inputs=ganInput, outputs=ganOutput) #creating combined model now that both networks are linked\n",
        "gan.compile(loss='sparse_categorical_crossentropy', optimizer=adam) #compiling combined model with custom optimizer\n",
        "\n",
        "#training network\n",
        "for i in range(1050): #number decided on by testing\n",
        "\n",
        "  print(\"Epoch\", i)\n",
        "  train_noise = np.array([np.random.normal(0,1,size=(784)) for i in range(128)], dtype=\"float\") #creating noise for the generator to predict on\n",
        "  refined_noise = model.predict(train_noise) #more noise created by generator\n",
        "\n",
        "  print(refined_noise.shape, realData.shape)\n",
        "  X = np.concatenate((realData, refined_noise)) #combining 8s and generated noise\n",
        "  y = np.concatenate((realAnswers, np.array([0 for i in range(128)]))) #creating labels\n",
        "\n",
        "  model2.trainable = True\n",
        "  model2.train_on_batch(X, y) #training the discriminator on the created data\n",
        "\n",
        "  #creating more training data for the combined network\n",
        "  X2 = np.array([np.random.normal(0,1,size=(784)) for i in range(256)], dtype=\"float\")\n",
        "  y2 = np.array([n for i in range(256)])\n",
        "\n",
        "  model2.trainable = False #making sure discriminator cannot be trained (we assume it will always be correct now)\n",
        "  gan.train_on_batch(X2, y2) #training the combined network on the training data (only adjusts the generator though)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 784) (128,)\n",
            "(784,)\n",
            "Epoch 0\n",
            "(128, 784) (128, 784)\n",
            "Epoch 1\n",
            "(128, 784) (128, 784)\n",
            "Epoch 2\n",
            "(128, 784) (128, 784)\n",
            "Epoch 3\n",
            "(128, 784) (128, 784)\n",
            "Epoch 4\n",
            "(128, 784) (128, 784)\n",
            "Epoch 5\n",
            "(128, 784) (128, 784)\n",
            "Epoch 6\n",
            "(128, 784) (128, 784)\n",
            "Epoch 7\n",
            "(128, 784) (128, 784)\n",
            "Epoch 8\n",
            "(128, 784) (128, 784)\n",
            "Epoch 9\n",
            "(128, 784) (128, 784)\n",
            "Epoch 10\n",
            "(128, 784) (128, 784)\n",
            "Epoch 11\n",
            "(128, 784) (128, 784)\n",
            "Epoch 12\n",
            "(128, 784) (128, 784)\n",
            "Epoch 13\n",
            "(128, 784) (128, 784)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-a562b109e18d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#training the discriminator on the created data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0;31m#creating more training data for the combined network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1725\u001b[0m                                                     class_weight)\n\u001b[1;32m   1726\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1727\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5937YXBs-aK3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "outputId": "f6e3b889-d840-43ff-e82f-957b3052584b"
      },
      "source": [
        "'''\n",
        "This piece of code makes the generator create 8s given noise (it's already trained) and dispays the result \n",
        "'''\n",
        "\n",
        "noise = np.random.normal(0, 1, size=[10000, 784])\n",
        "generatedImages = model.predict(noise)\n",
        "\n",
        "generatedImages = generatedImages.reshape(10000, 28, 28)\n",
        "\n",
        "plt.imshow(generatedImages[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}